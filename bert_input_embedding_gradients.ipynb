{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7354b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TORCHINDUCTOR_FX_GRAPH_CACHE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_AUTOGRAD_CACHE\"] = \"1\"\n",
    "\n",
    "from bert_experiments import experiments\n",
    "from compute_batch_bert_gradients import learn_embeddings\n",
    "\n",
    "# Run a single gradient descent step and collect the gradients\n",
    "args = experiments[\"one-step\"].copy()\n",
    "result = learn_embeddings(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5bee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from bert_util import get_token_distribution\n",
    "from bert_viz import plot_distribution\n",
    "\n",
    "HORSE_DISTRIBUTION = get_token_distribution([\"The animal that says neigh is a [MASK]\"])\n",
    "DOG_DISTRIBUTION = get_token_distribution([\"The animal that says bark is a [MASK]\"])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(10, 3), constrained_layout=True, sharey=True)\n",
    "plot_distribution(HORSE_DISTRIBUTION, 0, 6, axes[0])\n",
    "plot_distribution(DOG_DISTRIBUTION, 0, 6, axes[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dcb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sentences by gradient L2 norm\n",
    "import numpy as np\n",
    "from bert_viz import generate_saliency_html, viz_sentences_for_input_embed\n",
    "saliencies = np.linalg.norm(result.last_gradient, axis=2, ord=2)\n",
    "sentences = viz_sentences_for_input_embed(args, input_embeds=result.inputs_embeds_list[0])\n",
    "generate_saliency_html(args, sentences, saliencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tokens that have the highest cosine similarity to the gradient\n",
    "import numpy as np\n",
    "from bert_models import tokenizer, full_vocab_embedding_normalized\n",
    "from bert_viz import generate_saliency_html, viz_sentences_for_input_embed\n",
    "normalized_gradient = result.last_gradient / np.linalg.norm(result.last_gradient,axis=2,keepdims=True) # [B, N, D]\n",
    "cosine_sims = np.einsum('bnd,vd->bnv', normalized_gradient, full_vocab_embedding_normalized) # [B, N, V]\n",
    "closest_cosine_sim_vocab_token_idxs = np.argmax(cosine_sims,-1) # [B, N]\n",
    "closest_cosine_sim_vocab_tokens = [ tokenizer.convert_ids_to_tokens(idxs) for idxs in closest_cosine_sim_vocab_token_idxs ]\n",
    "saliencies = np.take_along_axis(cosine_sims, np.expand_dims(closest_cosine_sim_vocab_token_idxs, axis=-1), axis=-1)\n",
    "generate_saliency_html(args, closest_cosine_sim_vocab_tokens, saliencies, mask_token=\"___\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee807e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tokens that are pointed to by the gradient from the initial input embedding\n",
    "from bert_viz import viz_sentences_for_input_embed, viz_sentences_for_input_embed\n",
    "gradient_diff = result.last_gradient - result.inputs_embeds_list[0] # [B, N, D]\n",
    "gradient_diff_normalized = gradient_diff / np.linalg.norm(gradient_diff, axis=2, keepdims=True) # [B, N, D]\n",
    "cosine_sims = np.einsum('bnd,vd->bnv', normalized_gradient, full_vocab_embedding_normalized) # [B, N, V]\n",
    "nearest_token_ids = np.argmax(cosine_sims,-1) # [B, N]\n",
    "nearest_tokens = [ tokenizer.convert_ids_to_tokens(nearest_token_ids[i]) for i in range(len(nearest_token_ids)) ]\n",
    "saliencies = np.take_along_axis(cosine_sims, np.expand_dims(closest_cosine_sim_vocab_token_idxs, axis=-1), axis=-1)\n",
    "generate_saliency_html(args, nearest_tokens, saliencies, mask_token=\"___\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_experiments import experiments\n",
    "from compute_batch_bert_gradients import learn_embeddings\n",
    "from bert_util import collect_probability_path\n",
    "\n",
    "\n",
    "# Gradient descent dog->horse, 250 steps\n",
    "args = experiments[\"horse\"].copy()\n",
    "result = learn_embeddings(args)\n",
    "probs_path = collect_probability_path(args, result.inputs_embeds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_animate import animate_kl_divergences\n",
    "animate_kl_divergences(args, probs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bert_viz import animate_sentence_level_L2_distances\n",
    "from bert_animate import animate_sentence_level_L2_distances\n",
    "animate_sentence_level_L2_distances(args, result.inputs_embeds_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39171fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_animate import animate_token_level_L2_distances\n",
    "animate_token_level_L2_distances(args, result.inputs_embeds_list, sentence_idx = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdfdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_experiments import experiments\n",
    "from compute_batch_bert_gradients import learn_embeddings\n",
    "# Try 'eng-random-embedding' to help validate our \"close to everywhere global minima\" hypothesis\n",
    "args = experiments['eng'].copy()\n",
    "result = learn_embeddings(args)\n",
    "probs_path = collect_probability_path(args, result.inputs_embeds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_viz import display_gradient_displacement\n",
    "display_gradient_displacement(args, result.inputs_embeds_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tokens that are pointed to by the gradient from the initial input embedding\n",
    "import importlib, bert_viz as viz\n",
    "importlib.reload(viz)\n",
    "#from bert_viz import viz_sentences_for_input_embed, viz_sentences_for_input_embed\n",
    "gradient_diff = result.last_gradient - result.inputs_embeds_list[0] # [B, N, D]\n",
    "gradient_diff_normalized = gradient_diff / np.linalg.norm(gradient_diff, axis=2, keepdims=True) # [B, N, D]\n",
    "cosine_sims = np.einsum('bnd,vd->bnv', normalized_gradient, full_vocab_embedding_normalized) # [B, N, V]\n",
    "nearest_token_ids = np.argmax(cosine_sims,-1) # [B, N]\n",
    "nearest_tokens = [ tokenizer.convert_ids_to_tokens(nearest_token_ids[i]) for i in range(len(nearest_token_ids)) ]\n",
    "saliencies = np.take_along_axis(cosine_sims, np.expand_dims(closest_cosine_sim_vocab_token_idxs, axis=-1), axis=-1)\n",
    "print(nearest_tokens)\n",
    "viz.generate_saliency_html(args, nearest_tokens, saliencies, mask_token=\"____\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60234a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render BERT attention viz\n",
    "from bert_viz import viz_bert\n",
    "viz_bert(args, result.inputs_embeds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d27cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_IDX = 0\n",
    "from compute_batch_bert_gradients import tokenizer\n",
    "\n",
    "def clean_token(token):\n",
    "    return token.replace(\"Ġ\", \" \")\n",
    "\n",
    "\n",
    "special_tokens = set(['[CLS]', '[SEP]', '[PAD]'])\n",
    "def clean_sentence(sentence):\n",
    "    string = \"\"\n",
    "    for token in sentence:\n",
    "        if token in special_tokens:\n",
    "            continue\n",
    "        if token == '[MASK]':\n",
    "            token = ' ___'\n",
    "        token = token.replace(\"Ġ\", \" \")\n",
    "        string += token\n",
    "    return string\n",
    "\n",
    "start_inputs_embeds = input_embeds_list[0]\n",
    "start_sentences = [ clean_sentence(sentence) for sentence in invert_embeddings(start_inputs_embeds)[1] ]\n",
    "final_inputs_embeds = input_embeds_list[-1]\n",
    "end_sentences = [ clean_sentence(sentence) for sentence in invert_embeddings(final_inputs_embeds)[1] ]\n",
    "\n",
    "\n",
    "target_probs = args.target_probabilities # [N,V]\n",
    "top_target_prob_idxs = np.argsort(-target_probs, axis = -1)[:,:10]\n",
    "top_target_tokens = [ clean_token(token) for token in  tokenizer.convert_ids_to_tokens(top_target_prob_idxs[SELECTED_IDX,:]) ]\n",
    "top_target_probs = np.take_along_axis(target_probs, top_target_prob_idxs, axis = -1)\n",
    "\n",
    "np_mask_positions = np.asarray(args.mask_positions)\n",
    "\n",
    "\n",
    "masked_token_indexed_prob_paths = np.take_along_axis(probs_path, np.expand_dims(top_target_prob_idxs, axis = 0), axis = 2)\n",
    "selected_masked_token_indexed_prob_paths = masked_token_indexed_prob_paths[:,SELECTED_IDX,:]\n",
    "selected_top_target_probs = top_target_probs[SELECTED_IDX,:]\n",
    "selected_source_sentence = start_sentences[SELECTED_IDX]\n",
    "selected_target_sentence = \"The animal that says neigh is is a [ horse]\"#start_sentences[args.sentence_permutation[SELECTED_IDX]]\n",
    "\n",
    "\n",
    "selected_sentence_top_token_path = np.argsort(-probs_path[:,SELECTED_IDX,:], axis=1)[:,0]\n",
    "selected_sentence_top_token_path = [ clean_token(token) for token in tokenizer.convert_ids_to_tokens(selected_sentence_top_token_path)]\n",
    "\n",
    "\n",
    "sentences_path = [ clean_sentence(invert_embeddings(input_embeds_list[i][[SELECTED_IDX],...])[1][0]) for i in range(len(input_embeds_list)) ]\n",
    "\n",
    "selected_top_target_token = clean_token(tokenizer.convert_ids_to_tokens(top_target_prob_idxs[SELECTED_IDX,:1])[0])\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "def init():\n",
    "    ax.clear()\n",
    "    return []\n",
    "\n",
    "def animate(i):\n",
    "    ax.clear()\n",
    "    probs = selected_masked_token_indexed_prob_paths[i]\n",
    "    positions = range(len(probs))\n",
    "    ax.bar(positions, selected_top_target_probs, color='gray', alpha = 0.25)\n",
    "    bars = ax.bar(positions, probs, color='skyblue')\n",
    "    ax.set_ylim(0, selected_masked_token_indexed_prob_paths.max() * 1.1)\n",
    "    ax.set_xlabel('Tokens')\n",
    "    ax.set_ylabel('Probability')\n",
    "    # Token Probabilities at Step {i} \\n\n",
    "    ax.set_title(f'\"{sentences_path[i].replace(\"___\", f\"[{selected_sentence_top_token_path[i]}]\")}\"')\n",
    "    # \\nTarget: \"{selected_target_sentence.replace(\"___\", f\"[{selected_top_target_token}]\")}\"')\n",
    "    ax.set_xticks(range(len(top_target_tokens)))\n",
    "    ax.set_xticklabels(labels=top_target_tokens)\n",
    "    return bars\n",
    "\n",
    "ani = FuncAnimation(fig, animate, frames=selected_masked_token_indexed_prob_paths.shape[0], \n",
    "                   init_func=init, blit=True, interval=50)\n",
    "plt.close()  # Prevents duplicate display in Jupyter\n",
    "HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3aedf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c80bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rope-move-_krKvzaN-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
