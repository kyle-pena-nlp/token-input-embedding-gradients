{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7354b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TORCHINDUCTOR_FX_GRAPH_CACHE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_AUTOGRAD_CACHE\"] = \"1\"\n",
    "\n",
    "from llama_experiments import experiments\n",
    "from compute_batch_llama_gradients import learn_embeddings\n",
    "\n",
    "# Run a single gradient descent step and collect the gradients\n",
    "args = experiments[\"one-step\"].copy()\n",
    "result = learn_embeddings(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dcb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From one-step run.\n",
    "# Display sentences by gradient L2 norm\n",
    "import numpy as np\n",
    "from llama_viz import generate_saliency_html, viz_sentences_for_input_embed\n",
    "saliencies = np.linalg.norm(result.last_gradient, axis=2, ord=2)\n",
    "sentences = viz_sentences_for_input_embed(args, input_embeds=result.inputs_embeds_list[0])\n",
    "generate_saliency_html(args, sentences, saliencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From one-step run.\n",
    "# Display tokens that have the highest cosine similarity to the gradient\n",
    "import numpy as np\n",
    "from llama_models import tokenizer, full_vocab_embedding_normalized\n",
    "from llama_viz import generate_saliency_html, viz_sentences_for_input_embed\n",
    "normalized_gradient = result.last_gradient / np.linalg.norm(result.last_gradient,axis=2,keepdims=True) # [B, N, D]\n",
    "cosine_sims = np.einsum('bnd,vd->bnv', normalized_gradient, full_vocab_embedding_normalized) # [B, N, V]\n",
    "closest_cosine_sim_vocab_token_idxs = np.argmax(cosine_sims,-1) # [B, N]\n",
    "closest_cosine_sim_vocab_tokens = [ tokenizer.convert_ids_to_tokens(idxs) for idxs in closest_cosine_sim_vocab_token_idxs ]\n",
    "saliencies = np.take_along_axis(cosine_sims, np.expand_dims(closest_cosine_sim_vocab_token_idxs, axis=-1), axis=-1)\n",
    "generate_saliency_html(args, closest_cosine_sim_vocab_tokens, saliencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee807e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From one-step run\n",
    "# Display tokens that are pointed to by the gradient from the initial input embedding\n",
    "from llama_viz import viz_sentences_for_input_embed, viz_sentences_for_input_embed\n",
    "from llama_models import tokenizer, full_vocab_embedding_normalized\n",
    "gradient_diff = result.last_gradient - result.inputs_embeds_list[0] # [B, N, D]\n",
    "gradient_diff_normalized = gradient_diff / np.linalg.norm(gradient_diff, axis=2, keepdims=True) # [B, N, D]\n",
    "cosine_sims = np.einsum('bnd,vd->bnv', gradient_diff_normalized, full_vocab_embedding_normalized) # [B, N, V]\n",
    "nearest_token_ids = np.argmax(cosine_sims,-1) # [B, N]\n",
    "nearest_tokens = [ tokenizer.convert_ids_to_tokens(nearest_token_ids[i]) for i in range(len(nearest_token_ids)) ]\n",
    "saliencies = np.take_along_axis(cosine_sims, np.expand_dims(nearest_token_ids, axis=-1), axis=-1)\n",
    "generate_saliency_html(args, nearest_tokens, saliencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_experiments import experiments, HORSE_DISTRIBUTION\n",
    "from compute_batch_llama_gradients import learn_embeddings, BatchArgs\n",
    "from llama_util import collect_probability_path\n",
    "import scipy.stats \n",
    "\n",
    "print(scipy.stats.entropy(HORSE_DISTRIBUTION[0]))\n",
    "\n",
    "args = BatchArgs(steps = 1000,\n",
    "        num_examples = 64,\n",
    "        examples_filepath=\"eng_sentences.tsv\",\n",
    "        example_stride=10,\n",
    "        learning_rate = 1e-3,\n",
    "        target_probabilities=np.tile(HORSE_DISTRIBUTION, (64,1)))\n",
    "\n",
    "# Gradient descent dog->horse, 250 steps\n",
    "result = learn_embeddings(args)\n",
    "probs_path = collect_probability_path(args, result.inputs_embeds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bert_viz import animate_sentence_level_L2_distances\n",
    "from llama_animate import animate_sentence_level_L2_distances\n",
    "animate_sentence_level_L2_distances(args, result.inputs_embeds_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39171fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_animate import animate_token_level_L2_distances\n",
    "animate_token_level_L2_distances(args, result.inputs_embeds_list, sentence_idx = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdfdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_experiments import experiments\n",
    "from llama_util import collect_probability_path\n",
    "from compute_batch_llama_gradients import learn_embeddings\n",
    "from llama_util import compute_divergence\n",
    "# Try 'eng-random-embedding' to help validate our \"close to everywhere global minima\" hypothesis\n",
    "args = experiments['eng'].copy()\n",
    "args.steps = 250\n",
    "result = learn_embeddings(args)\n",
    "probs_path = collect_probability_path(args, result.inputs_embeds_list)\n",
    "print(\"Starting Divergence\", compute_divergence(args, result.inputs_embeds_list[0], args.target_probabilities))\n",
    "print(\"Ending Divergence\", compute_divergence(args, result.inputs_embeds_list[-1], args.target_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a96f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_viz import viz_sentence_changes\n",
    "from IPython.display import display\n",
    "import importlib, llama_viz\n",
    "importlib.reload(llama_viz)\n",
    "viz_sentence_changes = llama_viz.viz_sentence_changes\n",
    "display(viz_sentence_changes(args, result.inputs_embeds_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c520916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, llama_animate\n",
    "importlib.reload(llama_animate)\n",
    "animate_prob_distr_path = llama_animate.animate_prob_distr_path\n",
    "animate_prob_distr_path(args, result.inputs_embeds_list, probs_path, SELECTED_IDX = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, llama_viz\n",
    "importlib.reload(llama_viz)\n",
    "llama_viz.display_gradient_displacement(args, result.inputs_embeds_list, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_animate import animate_kl_divergences\n",
    "animate_kl_divergences(args, probs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bert_viz import animate_sentence_level_L2_distances\n",
    "from llama_animate import animate_sentence_level_L2_distances\n",
    "animate_sentence_level_L2_distances(args, result.inputs_embeds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sentences by gradient L2 norm\n",
    "import numpy as np\n",
    "from llama_viz import generate_saliency_html, viz_sentences_for_input_embed\n",
    "saliencies = np.linalg.norm(result.last_gradient, axis=2, ord=2)\n",
    "sentences = viz_sentences_for_input_embed(args, input_embeds=result.inputs_embeds_list[0])\n",
    "generate_saliency_html(args, sentences, saliencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tokens that are pointed to by the gradient from the initial input embedding\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import importlib, llama_viz as viz\n",
    "from llama_models import tokenizer, full_vocab_embedding_normalized\n",
    "importlib.reload(viz)\n",
    "#from bert_viz import viz_sentences_for_input_embed, viz_sentences_for_input_embed\n",
    "gradient_diff = result.last_gradient - result.inputs_embeds_list[0] # [B, N, D]\n",
    "gradient_diff_normalized = gradient_diff / np.linalg.norm(gradient_diff, axis=2, keepdims=True) # [B, N, D]\n",
    "cosine_sims = np.einsum('bnd,vd->bnv', gradient_diff_normalized, full_vocab_embedding_normalized) # [B, N, V]\n",
    "nearest_token_ids = np.argmax(cosine_sims,-1) # [B, N]\n",
    "nearest_tokens = [ tokenizer.convert_ids_to_tokens(nearest_token_ids[i]) for i in range(len(nearest_token_ids)) ]\n",
    "saliencies = np.take_along_axis(cosine_sims, np.expand_dims(nearest_token_ids, axis=-1), axis=-1)\n",
    "display(viz.generate_saliency_html(args, nearest_tokens, saliencies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d27cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3aedf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c80bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rope-move-_krKvzaN-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
